{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining : Microarray Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Packages***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold, LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from ReliefF import ReliefF\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from yellowbrick.features.pca import PCADecomposition\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/colonTumor.csv\")\n",
    "data_length = data.shape[1]\n",
    "data_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = []\n",
    "for i in range(data_length):\n",
    "    column.append(\"atribut\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atribut0</th>\n",
       "      <th>atribut1</th>\n",
       "      <th>atribut2</th>\n",
       "      <th>atribut3</th>\n",
       "      <th>atribut4</th>\n",
       "      <th>atribut5</th>\n",
       "      <th>atribut6</th>\n",
       "      <th>atribut7</th>\n",
       "      <th>atribut8</th>\n",
       "      <th>atribut9</th>\n",
       "      <th>...</th>\n",
       "      <th>atribut1991</th>\n",
       "      <th>atribut1992</th>\n",
       "      <th>atribut1993</th>\n",
       "      <th>atribut1994</th>\n",
       "      <th>atribut1995</th>\n",
       "      <th>atribut1996</th>\n",
       "      <th>atribut1997</th>\n",
       "      <th>atribut1998</th>\n",
       "      <th>atribut1999</th>\n",
       "      <th>atribut2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8589.4163</td>\n",
       "      <td>5468.2409</td>\n",
       "      <td>4263.4075</td>\n",
       "      <td>4064.9357</td>\n",
       "      <td>1997.8929</td>\n",
       "      <td>5282.3250</td>\n",
       "      <td>2169.7200</td>\n",
       "      <td>2773.4212</td>\n",
       "      <td>7526.3862</td>\n",
       "      <td>4607.6762</td>\n",
       "      <td>...</td>\n",
       "      <td>67.56125</td>\n",
       "      <td>259.91250</td>\n",
       "      <td>138.89875</td>\n",
       "      <td>88.23250</td>\n",
       "      <td>39.667857</td>\n",
       "      <td>67.82875</td>\n",
       "      <td>75.67750</td>\n",
       "      <td>83.52250</td>\n",
       "      <td>28.70125</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9164.2537</td>\n",
       "      <td>6719.5295</td>\n",
       "      <td>4883.4487</td>\n",
       "      <td>3718.1589</td>\n",
       "      <td>2015.2214</td>\n",
       "      <td>5569.9071</td>\n",
       "      <td>3849.0588</td>\n",
       "      <td>2793.3875</td>\n",
       "      <td>7017.7338</td>\n",
       "      <td>4802.2524</td>\n",
       "      <td>...</td>\n",
       "      <td>92.23875</td>\n",
       "      <td>96.27625</td>\n",
       "      <td>150.59000</td>\n",
       "      <td>82.23750</td>\n",
       "      <td>85.033333</td>\n",
       "      <td>152.19500</td>\n",
       "      <td>186.56750</td>\n",
       "      <td>44.47250</td>\n",
       "      <td>16.77375</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3825.7050</td>\n",
       "      <td>6970.3614</td>\n",
       "      <td>5369.9688</td>\n",
       "      <td>4705.6500</td>\n",
       "      <td>1166.5536</td>\n",
       "      <td>1572.1679</td>\n",
       "      <td>1325.4025</td>\n",
       "      <td>1472.2587</td>\n",
       "      <td>3296.9512</td>\n",
       "      <td>2786.5821</td>\n",
       "      <td>...</td>\n",
       "      <td>82.71500</td>\n",
       "      <td>31.10250</td>\n",
       "      <td>193.92000</td>\n",
       "      <td>76.97250</td>\n",
       "      <td>224.620240</td>\n",
       "      <td>31.22500</td>\n",
       "      <td>42.65625</td>\n",
       "      <td>16.09250</td>\n",
       "      <td>15.15625</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6246.4487</td>\n",
       "      <td>7823.5341</td>\n",
       "      <td>5955.8350</td>\n",
       "      <td>3975.5643</td>\n",
       "      <td>2002.6131</td>\n",
       "      <td>2130.5429</td>\n",
       "      <td>1531.1425</td>\n",
       "      <td>1714.6312</td>\n",
       "      <td>3869.7850</td>\n",
       "      <td>4989.4071</td>\n",
       "      <td>...</td>\n",
       "      <td>41.68375</td>\n",
       "      <td>5.92500</td>\n",
       "      <td>183.00625</td>\n",
       "      <td>74.52875</td>\n",
       "      <td>67.710714</td>\n",
       "      <td>48.33875</td>\n",
       "      <td>42.52000</td>\n",
       "      <td>49.98250</td>\n",
       "      <td>16.08500</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3230.3287</td>\n",
       "      <td>3694.4500</td>\n",
       "      <td>3400.7400</td>\n",
       "      <td>3463.5857</td>\n",
       "      <td>2181.4202</td>\n",
       "      <td>2922.7821</td>\n",
       "      <td>2069.2463</td>\n",
       "      <td>2948.5750</td>\n",
       "      <td>3303.3712</td>\n",
       "      <td>3109.4131</td>\n",
       "      <td>...</td>\n",
       "      <td>76.60375</td>\n",
       "      <td>161.35000</td>\n",
       "      <td>61.70125</td>\n",
       "      <td>54.56375</td>\n",
       "      <td>223.359520</td>\n",
       "      <td>73.09875</td>\n",
       "      <td>57.59875</td>\n",
       "      <td>7.48875</td>\n",
       "      <td>31.81250</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    atribut0   atribut1   atribut2   atribut3   atribut4   atribut5  \\\n",
       "0  8589.4163  5468.2409  4263.4075  4064.9357  1997.8929  5282.3250   \n",
       "1  9164.2537  6719.5295  4883.4487  3718.1589  2015.2214  5569.9071   \n",
       "2  3825.7050  6970.3614  5369.9688  4705.6500  1166.5536  1572.1679   \n",
       "3  6246.4487  7823.5341  5955.8350  3975.5643  2002.6131  2130.5429   \n",
       "4  3230.3287  3694.4500  3400.7400  3463.5857  2181.4202  2922.7821   \n",
       "\n",
       "    atribut6   atribut7   atribut8   atribut9  ...  atribut1991  atribut1992  \\\n",
       "0  2169.7200  2773.4212  7526.3862  4607.6762  ...     67.56125    259.91250   \n",
       "1  3849.0588  2793.3875  7017.7338  4802.2524  ...     92.23875     96.27625   \n",
       "2  1325.4025  1472.2587  3296.9512  2786.5821  ...     82.71500     31.10250   \n",
       "3  1531.1425  1714.6312  3869.7850  4989.4071  ...     41.68375      5.92500   \n",
       "4  2069.2463  2948.5750  3303.3712  3109.4131  ...     76.60375    161.35000   \n",
       "\n",
       "   atribut1993  atribut1994  atribut1995  atribut1996  atribut1997  \\\n",
       "0    138.89875     88.23250    39.667857     67.82875     75.67750   \n",
       "1    150.59000     82.23750    85.033333    152.19500    186.56750   \n",
       "2    193.92000     76.97250   224.620240     31.22500     42.65625   \n",
       "3    183.00625     74.52875    67.710714     48.33875     42.52000   \n",
       "4     61.70125     54.56375   223.359520     73.09875     57.59875   \n",
       "\n",
       "   atribut1998  atribut1999  atribut2000  \n",
       "0     83.52250     28.70125     negative  \n",
       "1     44.47250     16.77375     positive  \n",
       "2     16.09250     15.15625     negative  \n",
       "3     49.98250     16.08500     positive  \n",
       "4      7.48875     31.81250     negative  \n",
       "\n",
       "[5 rows x 2001 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/colonTumor.csv\", header=None, names=column)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atribut0</th>\n",
       "      <th>atribut1</th>\n",
       "      <th>atribut2</th>\n",
       "      <th>atribut3</th>\n",
       "      <th>atribut4</th>\n",
       "      <th>atribut5</th>\n",
       "      <th>atribut6</th>\n",
       "      <th>atribut7</th>\n",
       "      <th>atribut8</th>\n",
       "      <th>atribut9</th>\n",
       "      <th>...</th>\n",
       "      <th>atribut1991</th>\n",
       "      <th>atribut1992</th>\n",
       "      <th>atribut1993</th>\n",
       "      <th>atribut1994</th>\n",
       "      <th>atribut1995</th>\n",
       "      <th>atribut1996</th>\n",
       "      <th>atribut1997</th>\n",
       "      <th>atribut1998</th>\n",
       "      <th>atribut1999</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8589.4163</td>\n",
       "      <td>5468.2409</td>\n",
       "      <td>4263.4075</td>\n",
       "      <td>4064.9357</td>\n",
       "      <td>1997.8929</td>\n",
       "      <td>5282.3250</td>\n",
       "      <td>2169.7200</td>\n",
       "      <td>2773.4212</td>\n",
       "      <td>7526.3862</td>\n",
       "      <td>4607.6762</td>\n",
       "      <td>...</td>\n",
       "      <td>67.56125</td>\n",
       "      <td>259.91250</td>\n",
       "      <td>138.89875</td>\n",
       "      <td>88.23250</td>\n",
       "      <td>39.667857</td>\n",
       "      <td>67.82875</td>\n",
       "      <td>75.67750</td>\n",
       "      <td>83.52250</td>\n",
       "      <td>28.70125</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9164.2537</td>\n",
       "      <td>6719.5295</td>\n",
       "      <td>4883.4487</td>\n",
       "      <td>3718.1589</td>\n",
       "      <td>2015.2214</td>\n",
       "      <td>5569.9071</td>\n",
       "      <td>3849.0588</td>\n",
       "      <td>2793.3875</td>\n",
       "      <td>7017.7338</td>\n",
       "      <td>4802.2524</td>\n",
       "      <td>...</td>\n",
       "      <td>92.23875</td>\n",
       "      <td>96.27625</td>\n",
       "      <td>150.59000</td>\n",
       "      <td>82.23750</td>\n",
       "      <td>85.033333</td>\n",
       "      <td>152.19500</td>\n",
       "      <td>186.56750</td>\n",
       "      <td>44.47250</td>\n",
       "      <td>16.77375</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3825.7050</td>\n",
       "      <td>6970.3614</td>\n",
       "      <td>5369.9688</td>\n",
       "      <td>4705.6500</td>\n",
       "      <td>1166.5536</td>\n",
       "      <td>1572.1679</td>\n",
       "      <td>1325.4025</td>\n",
       "      <td>1472.2587</td>\n",
       "      <td>3296.9512</td>\n",
       "      <td>2786.5821</td>\n",
       "      <td>...</td>\n",
       "      <td>82.71500</td>\n",
       "      <td>31.10250</td>\n",
       "      <td>193.92000</td>\n",
       "      <td>76.97250</td>\n",
       "      <td>224.620240</td>\n",
       "      <td>31.22500</td>\n",
       "      <td>42.65625</td>\n",
       "      <td>16.09250</td>\n",
       "      <td>15.15625</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6246.4487</td>\n",
       "      <td>7823.5341</td>\n",
       "      <td>5955.8350</td>\n",
       "      <td>3975.5643</td>\n",
       "      <td>2002.6131</td>\n",
       "      <td>2130.5429</td>\n",
       "      <td>1531.1425</td>\n",
       "      <td>1714.6312</td>\n",
       "      <td>3869.7850</td>\n",
       "      <td>4989.4071</td>\n",
       "      <td>...</td>\n",
       "      <td>41.68375</td>\n",
       "      <td>5.92500</td>\n",
       "      <td>183.00625</td>\n",
       "      <td>74.52875</td>\n",
       "      <td>67.710714</td>\n",
       "      <td>48.33875</td>\n",
       "      <td>42.52000</td>\n",
       "      <td>49.98250</td>\n",
       "      <td>16.08500</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3230.3287</td>\n",
       "      <td>3694.4500</td>\n",
       "      <td>3400.7400</td>\n",
       "      <td>3463.5857</td>\n",
       "      <td>2181.4202</td>\n",
       "      <td>2922.7821</td>\n",
       "      <td>2069.2463</td>\n",
       "      <td>2948.5750</td>\n",
       "      <td>3303.3712</td>\n",
       "      <td>3109.4131</td>\n",
       "      <td>...</td>\n",
       "      <td>76.60375</td>\n",
       "      <td>161.35000</td>\n",
       "      <td>61.70125</td>\n",
       "      <td>54.56375</td>\n",
       "      <td>223.359520</td>\n",
       "      <td>73.09875</td>\n",
       "      <td>57.59875</td>\n",
       "      <td>7.48875</td>\n",
       "      <td>31.81250</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    atribut0   atribut1   atribut2   atribut3   atribut4   atribut5  \\\n",
       "0  8589.4163  5468.2409  4263.4075  4064.9357  1997.8929  5282.3250   \n",
       "1  9164.2537  6719.5295  4883.4487  3718.1589  2015.2214  5569.9071   \n",
       "2  3825.7050  6970.3614  5369.9688  4705.6500  1166.5536  1572.1679   \n",
       "3  6246.4487  7823.5341  5955.8350  3975.5643  2002.6131  2130.5429   \n",
       "4  3230.3287  3694.4500  3400.7400  3463.5857  2181.4202  2922.7821   \n",
       "\n",
       "    atribut6   atribut7   atribut8   atribut9  ...  atribut1991  atribut1992  \\\n",
       "0  2169.7200  2773.4212  7526.3862  4607.6762  ...     67.56125    259.91250   \n",
       "1  3849.0588  2793.3875  7017.7338  4802.2524  ...     92.23875     96.27625   \n",
       "2  1325.4025  1472.2587  3296.9512  2786.5821  ...     82.71500     31.10250   \n",
       "3  1531.1425  1714.6312  3869.7850  4989.4071  ...     41.68375      5.92500   \n",
       "4  2069.2463  2948.5750  3303.3712  3109.4131  ...     76.60375    161.35000   \n",
       "\n",
       "   atribut1993  atribut1994  atribut1995  atribut1996  atribut1997  \\\n",
       "0    138.89875     88.23250    39.667857     67.82875     75.67750   \n",
       "1    150.59000     82.23750    85.033333    152.19500    186.56750   \n",
       "2    193.92000     76.97250   224.620240     31.22500     42.65625   \n",
       "3    183.00625     74.52875    67.710714     48.33875     42.52000   \n",
       "4     61.70125     54.56375   223.359520     73.09875     57.59875   \n",
       "\n",
       "   atribut1998  atribut1999    status  \n",
       "0     83.52250     28.70125  negative  \n",
       "1     44.47250     16.77375  positive  \n",
       "2     16.09250     15.15625  negative  \n",
       "3     49.98250     16.08500  positive  \n",
       "4      7.48875     31.81250  negative  \n",
       "\n",
       "[5 rows x 2001 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.rename(columns={'atribut2000': 'status'})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atribut0</th>\n",
       "      <th>atribut1</th>\n",
       "      <th>atribut2</th>\n",
       "      <th>atribut3</th>\n",
       "      <th>atribut4</th>\n",
       "      <th>atribut5</th>\n",
       "      <th>atribut6</th>\n",
       "      <th>atribut7</th>\n",
       "      <th>atribut8</th>\n",
       "      <th>atribut9</th>\n",
       "      <th>...</th>\n",
       "      <th>atribut1991</th>\n",
       "      <th>atribut1992</th>\n",
       "      <th>atribut1993</th>\n",
       "      <th>atribut1994</th>\n",
       "      <th>atribut1995</th>\n",
       "      <th>atribut1996</th>\n",
       "      <th>atribut1997</th>\n",
       "      <th>atribut1998</th>\n",
       "      <th>atribut1999</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8589.4163</td>\n",
       "      <td>5468.2409</td>\n",
       "      <td>4263.4075</td>\n",
       "      <td>4064.9357</td>\n",
       "      <td>1997.8929</td>\n",
       "      <td>5282.3250</td>\n",
       "      <td>2169.7200</td>\n",
       "      <td>2773.4212</td>\n",
       "      <td>7526.3862</td>\n",
       "      <td>4607.6762</td>\n",
       "      <td>...</td>\n",
       "      <td>67.56125</td>\n",
       "      <td>259.91250</td>\n",
       "      <td>138.89875</td>\n",
       "      <td>88.23250</td>\n",
       "      <td>39.667857</td>\n",
       "      <td>67.82875</td>\n",
       "      <td>75.67750</td>\n",
       "      <td>83.52250</td>\n",
       "      <td>28.70125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9164.2537</td>\n",
       "      <td>6719.5295</td>\n",
       "      <td>4883.4487</td>\n",
       "      <td>3718.1589</td>\n",
       "      <td>2015.2214</td>\n",
       "      <td>5569.9071</td>\n",
       "      <td>3849.0588</td>\n",
       "      <td>2793.3875</td>\n",
       "      <td>7017.7338</td>\n",
       "      <td>4802.2524</td>\n",
       "      <td>...</td>\n",
       "      <td>92.23875</td>\n",
       "      <td>96.27625</td>\n",
       "      <td>150.59000</td>\n",
       "      <td>82.23750</td>\n",
       "      <td>85.033333</td>\n",
       "      <td>152.19500</td>\n",
       "      <td>186.56750</td>\n",
       "      <td>44.47250</td>\n",
       "      <td>16.77375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3825.7050</td>\n",
       "      <td>6970.3614</td>\n",
       "      <td>5369.9688</td>\n",
       "      <td>4705.6500</td>\n",
       "      <td>1166.5536</td>\n",
       "      <td>1572.1679</td>\n",
       "      <td>1325.4025</td>\n",
       "      <td>1472.2587</td>\n",
       "      <td>3296.9512</td>\n",
       "      <td>2786.5821</td>\n",
       "      <td>...</td>\n",
       "      <td>82.71500</td>\n",
       "      <td>31.10250</td>\n",
       "      <td>193.92000</td>\n",
       "      <td>76.97250</td>\n",
       "      <td>224.620240</td>\n",
       "      <td>31.22500</td>\n",
       "      <td>42.65625</td>\n",
       "      <td>16.09250</td>\n",
       "      <td>15.15625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6246.4487</td>\n",
       "      <td>7823.5341</td>\n",
       "      <td>5955.8350</td>\n",
       "      <td>3975.5643</td>\n",
       "      <td>2002.6131</td>\n",
       "      <td>2130.5429</td>\n",
       "      <td>1531.1425</td>\n",
       "      <td>1714.6312</td>\n",
       "      <td>3869.7850</td>\n",
       "      <td>4989.4071</td>\n",
       "      <td>...</td>\n",
       "      <td>41.68375</td>\n",
       "      <td>5.92500</td>\n",
       "      <td>183.00625</td>\n",
       "      <td>74.52875</td>\n",
       "      <td>67.710714</td>\n",
       "      <td>48.33875</td>\n",
       "      <td>42.52000</td>\n",
       "      <td>49.98250</td>\n",
       "      <td>16.08500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3230.3287</td>\n",
       "      <td>3694.4500</td>\n",
       "      <td>3400.7400</td>\n",
       "      <td>3463.5857</td>\n",
       "      <td>2181.4202</td>\n",
       "      <td>2922.7821</td>\n",
       "      <td>2069.2463</td>\n",
       "      <td>2948.5750</td>\n",
       "      <td>3303.3712</td>\n",
       "      <td>3109.4131</td>\n",
       "      <td>...</td>\n",
       "      <td>76.60375</td>\n",
       "      <td>161.35000</td>\n",
       "      <td>61.70125</td>\n",
       "      <td>54.56375</td>\n",
       "      <td>223.359520</td>\n",
       "      <td>73.09875</td>\n",
       "      <td>57.59875</td>\n",
       "      <td>7.48875</td>\n",
       "      <td>31.81250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    atribut0   atribut1   atribut2   atribut3   atribut4   atribut5  \\\n",
       "0  8589.4163  5468.2409  4263.4075  4064.9357  1997.8929  5282.3250   \n",
       "1  9164.2537  6719.5295  4883.4487  3718.1589  2015.2214  5569.9071   \n",
       "2  3825.7050  6970.3614  5369.9688  4705.6500  1166.5536  1572.1679   \n",
       "3  6246.4487  7823.5341  5955.8350  3975.5643  2002.6131  2130.5429   \n",
       "4  3230.3287  3694.4500  3400.7400  3463.5857  2181.4202  2922.7821   \n",
       "\n",
       "    atribut6   atribut7   atribut8   atribut9  ...  atribut1991  atribut1992  \\\n",
       "0  2169.7200  2773.4212  7526.3862  4607.6762  ...     67.56125    259.91250   \n",
       "1  3849.0588  2793.3875  7017.7338  4802.2524  ...     92.23875     96.27625   \n",
       "2  1325.4025  1472.2587  3296.9512  2786.5821  ...     82.71500     31.10250   \n",
       "3  1531.1425  1714.6312  3869.7850  4989.4071  ...     41.68375      5.92500   \n",
       "4  2069.2463  2948.5750  3303.3712  3109.4131  ...     76.60375    161.35000   \n",
       "\n",
       "   atribut1993  atribut1994  atribut1995  atribut1996  atribut1997  \\\n",
       "0    138.89875     88.23250    39.667857     67.82875     75.67750   \n",
       "1    150.59000     82.23750    85.033333    152.19500    186.56750   \n",
       "2    193.92000     76.97250   224.620240     31.22500     42.65625   \n",
       "3    183.00625     74.52875    67.710714     48.33875     42.52000   \n",
       "4     61.70125     54.56375   223.359520     73.09875     57.59875   \n",
       "\n",
       "   atribut1998  atribut1999  status  \n",
       "0     83.52250     28.70125       0  \n",
       "1     44.47250     16.77375       1  \n",
       "2     16.09250     15.15625       0  \n",
       "3     49.98250     16.08500       1  \n",
       "4      7.48875     31.81250       0  \n",
       "\n",
       "[5 rows x 2001 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['status'] == 'negative', ['status']] = 0\n",
    "data.loc[data['status'] == 'positive', ['status']] = 1\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perbandingan jumlah positive dan negative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['status'].value_counts().plot(kind='bar', color=\"r\")\n",
    "plt.title(\"Perbandingan jumlah positive dan negative\")\n",
    "plt.xlabel('Status')\n",
    "plt.ylabel('Total')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**statistic info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atribut0</th>\n",
       "      <th>atribut1</th>\n",
       "      <th>atribut2</th>\n",
       "      <th>atribut3</th>\n",
       "      <th>atribut4</th>\n",
       "      <th>atribut5</th>\n",
       "      <th>atribut6</th>\n",
       "      <th>atribut7</th>\n",
       "      <th>atribut8</th>\n",
       "      <th>atribut9</th>\n",
       "      <th>...</th>\n",
       "      <th>atribut1991</th>\n",
       "      <th>atribut1992</th>\n",
       "      <th>atribut1993</th>\n",
       "      <th>atribut1994</th>\n",
       "      <th>atribut1995</th>\n",
       "      <th>atribut1996</th>\n",
       "      <th>atribut1997</th>\n",
       "      <th>atribut1998</th>\n",
       "      <th>atribut1999</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7015.786710</td>\n",
       "      <td>4966.960015</td>\n",
       "      <td>4094.727879</td>\n",
       "      <td>3987.789284</td>\n",
       "      <td>2937.126113</td>\n",
       "      <td>4705.119302</td>\n",
       "      <td>3588.800323</td>\n",
       "      <td>2872.288631</td>\n",
       "      <td>4680.191160</td>\n",
       "      <td>4039.661953</td>\n",
       "      <td>...</td>\n",
       "      <td>100.227903</td>\n",
       "      <td>293.222722</td>\n",
       "      <td>124.653387</td>\n",
       "      <td>133.186935</td>\n",
       "      <td>184.136635</td>\n",
       "      <td>84.118387</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>53.251230</td>\n",
       "      <td>42.965827</td>\n",
       "      <td>0.354839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3092.970584</td>\n",
       "      <td>2188.890480</td>\n",
       "      <td>1818.080939</td>\n",
       "      <td>2019.086903</td>\n",
       "      <td>1356.932887</td>\n",
       "      <td>2400.848112</td>\n",
       "      <td>1872.106095</td>\n",
       "      <td>1122.365448</td>\n",
       "      <td>2417.239217</td>\n",
       "      <td>2018.044248</td>\n",
       "      <td>...</td>\n",
       "      <td>78.022712</td>\n",
       "      <td>179.249194</td>\n",
       "      <td>75.535838</td>\n",
       "      <td>101.372557</td>\n",
       "      <td>159.914871</td>\n",
       "      <td>86.182028</td>\n",
       "      <td>88.011866</td>\n",
       "      <td>38.462814</td>\n",
       "      <td>28.395175</td>\n",
       "      <td>0.482370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1914.677500</td>\n",
       "      <td>1383.488600</td>\n",
       "      <td>1269.648700</td>\n",
       "      <td>1186.030400</td>\n",
       "      <td>1166.553600</td>\n",
       "      <td>1087.750000</td>\n",
       "      <td>1062.697500</td>\n",
       "      <td>1026.477500</td>\n",
       "      <td>995.790000</td>\n",
       "      <td>974.815480</td>\n",
       "      <td>...</td>\n",
       "      <td>5.935000</td>\n",
       "      <td>5.925000</td>\n",
       "      <td>5.923750</td>\n",
       "      <td>5.916250</td>\n",
       "      <td>5.888095</td>\n",
       "      <td>5.878750</td>\n",
       "      <td>5.848750</td>\n",
       "      <td>5.842500</td>\n",
       "      <td>5.816250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4877.364700</td>\n",
       "      <td>3408.951150</td>\n",
       "      <td>2763.725900</td>\n",
       "      <td>2648.942875</td>\n",
       "      <td>1890.857125</td>\n",
       "      <td>2843.368775</td>\n",
       "      <td>2112.952150</td>\n",
       "      <td>2050.814025</td>\n",
       "      <td>2971.792775</td>\n",
       "      <td>2797.992525</td>\n",
       "      <td>...</td>\n",
       "      <td>48.409375</td>\n",
       "      <td>161.518125</td>\n",
       "      <td>69.121250</td>\n",
       "      <td>71.637500</td>\n",
       "      <td>85.905059</td>\n",
       "      <td>30.701875</td>\n",
       "      <td>53.881563</td>\n",
       "      <td>27.334375</td>\n",
       "      <td>24.610000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6274.612500</td>\n",
       "      <td>4738.280700</td>\n",
       "      <td>3890.338100</td>\n",
       "      <td>3451.498200</td>\n",
       "      <td>2666.075600</td>\n",
       "      <td>4416.617850</td>\n",
       "      <td>3438.180000</td>\n",
       "      <td>2844.960650</td>\n",
       "      <td>4088.735600</td>\n",
       "      <td>3823.017900</td>\n",
       "      <td>...</td>\n",
       "      <td>83.080625</td>\n",
       "      <td>251.883125</td>\n",
       "      <td>114.658125</td>\n",
       "      <td>104.271875</td>\n",
       "      <td>142.578570</td>\n",
       "      <td>56.385625</td>\n",
       "      <td>93.825625</td>\n",
       "      <td>45.358125</td>\n",
       "      <td>34.775000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8712.283175</td>\n",
       "      <td>6455.843200</td>\n",
       "      <td>5008.993750</td>\n",
       "      <td>4738.408950</td>\n",
       "      <td>3563.969325</td>\n",
       "      <td>6076.032125</td>\n",
       "      <td>4420.645350</td>\n",
       "      <td>3414.228475</td>\n",
       "      <td>6171.382800</td>\n",
       "      <td>4840.715500</td>\n",
       "      <td>...</td>\n",
       "      <td>117.208438</td>\n",
       "      <td>424.742500</td>\n",
       "      <td>177.730312</td>\n",
       "      <td>149.772188</td>\n",
       "      <td>212.019940</td>\n",
       "      <td>101.231250</td>\n",
       "      <td>145.074062</td>\n",
       "      <td>66.235312</td>\n",
       "      <td>54.697500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14876.407000</td>\n",
       "      <td>10152.273000</td>\n",
       "      <td>8605.043800</td>\n",
       "      <td>11248.680000</td>\n",
       "      <td>8093.875000</td>\n",
       "      <td>11222.682000</td>\n",
       "      <td>9939.246200</td>\n",
       "      <td>5917.026300</td>\n",
       "      <td>14144.835000</td>\n",
       "      <td>12307.913000</td>\n",
       "      <td>...</td>\n",
       "      <td>438.383750</td>\n",
       "      <td>902.572500</td>\n",
       "      <td>333.418750</td>\n",
       "      <td>464.930000</td>\n",
       "      <td>702.130950</td>\n",
       "      <td>405.600000</td>\n",
       "      <td>390.890000</td>\n",
       "      <td>197.220000</td>\n",
       "      <td>126.826250</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           atribut0      atribut1     atribut2      atribut3     atribut4  \\\n",
       "count     62.000000     62.000000    62.000000     62.000000    62.000000   \n",
       "mean    7015.786710   4966.960015  4094.727879   3987.789284  2937.126113   \n",
       "std     3092.970584   2188.890480  1818.080939   2019.086903  1356.932887   \n",
       "min     1914.677500   1383.488600  1269.648700   1186.030400  1166.553600   \n",
       "25%     4877.364700   3408.951150  2763.725900   2648.942875  1890.857125   \n",
       "50%     6274.612500   4738.280700  3890.338100   3451.498200  2666.075600   \n",
       "75%     8712.283175   6455.843200  5008.993750   4738.408950  3563.969325   \n",
       "max    14876.407000  10152.273000  8605.043800  11248.680000  8093.875000   \n",
       "\n",
       "           atribut5     atribut6     atribut7      atribut8      atribut9  \\\n",
       "count     62.000000    62.000000    62.000000     62.000000     62.000000   \n",
       "mean    4705.119302  3588.800323  2872.288631   4680.191160   4039.661953   \n",
       "std     2400.848112  1872.106095  1122.365448   2417.239217   2018.044248   \n",
       "min     1087.750000  1062.697500  1026.477500    995.790000    974.815480   \n",
       "25%     2843.368775  2112.952150  2050.814025   2971.792775   2797.992525   \n",
       "50%     4416.617850  3438.180000  2844.960650   4088.735600   3823.017900   \n",
       "75%     6076.032125  4420.645350  3414.228475   6171.382800   4840.715500   \n",
       "max    11222.682000  9939.246200  5917.026300  14144.835000  12307.913000   \n",
       "\n",
       "       ...  atribut1991  atribut1992  atribut1993  atribut1994  atribut1995  \\\n",
       "count  ...    62.000000    62.000000    62.000000    62.000000    62.000000   \n",
       "mean   ...   100.227903   293.222722   124.653387   133.186935   184.136635   \n",
       "std    ...    78.022712   179.249194    75.535838   101.372557   159.914871   \n",
       "min    ...     5.935000     5.925000     5.923750     5.916250     5.888095   \n",
       "25%    ...    48.409375   161.518125    69.121250    71.637500    85.905059   \n",
       "50%    ...    83.080625   251.883125   114.658125   104.271875   142.578570   \n",
       "75%    ...   117.208438   424.742500   177.730312   149.772188   212.019940   \n",
       "max    ...   438.383750   902.572500   333.418750   464.930000   702.130950   \n",
       "\n",
       "       atribut1996  atribut1997  atribut1998  atribut1999     status  \n",
       "count    62.000000    62.000000    62.000000    62.000000  62.000000  \n",
       "mean     84.118387   114.930000    53.251230    42.965827   0.354839  \n",
       "std      86.182028    88.011866    38.462814    28.395175   0.482370  \n",
       "min       5.878750     5.848750     5.842500     5.816250   0.000000  \n",
       "25%      30.701875    53.881563    27.334375    24.610000   0.000000  \n",
       "50%      56.385625    93.825625    45.358125    34.775000   0.000000  \n",
       "75%     101.231250   145.074062    66.235312    54.697500   1.000000  \n",
       "max     405.600000   390.890000   197.220000   126.826250   1.000000  \n",
       "\n",
       "[8 rows x 2001 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Target Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['status'],axis=1)\n",
    "y = data['status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scaler(X_train_, X_test_):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_minmax = scaler.fit_transform(X_train_)\n",
    "    X_train_ = pd.DataFrame(X_train_minmax, columns=X_train_.columns)\n",
    "    X_test_minmax = scaler.transform(X_test_)\n",
    "    X_test_ = pd.DataFrame(X_test_minmax, columns=X_test_.columns)\n",
    "    return X_train_,X_test_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering (K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(X_train_):\n",
    "    X_train_transpose = X_train_.transpose()\n",
    "    model_kme = KMeans(n_clusters= 6)\n",
    "    model_kme.fit(X_train_transpose)\n",
    "    pred_kme = model_kme.predict(X_train_transpose)\n",
    "    X_train_transpose_clustered = X_train_transpose.copy()\n",
    "    X_train_transpose_clustered['cluster'] = pred_kme \n",
    "    return X_train_transpose_clustered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relief Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_column(data_, data_selected, num_features_selected, left_columns):\n",
    "    for i in data_.columns:\n",
    "        if ((data_[i].values.tolist() == data_selected.iloc[:,num_features_selected].values.tolist()) and (i not in left_columns)):\n",
    "            column = i\n",
    "            break\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relief_method(X_train_transpose_clusteredx, X_train_,y_train_, label_cluster):\n",
    "    n_features_to_keep = 8\n",
    "    n_neighbors = X_train_.shape[0]-1\n",
    "    Train = pd.DataFrame()\n",
    "    for i in label_cluster:\n",
    "        X_train_transpose_clustered_ = X_train_transpose_clusteredx[X_train_transpose_clusteredx['cluster']==i]\n",
    "        X_train_clustered_ = X_train_transpose_clustered_.drop(['cluster'],axis=1).transpose()\n",
    "        fs = ReliefF(n_neighbors=n_neighbors, n_features_to_keep=n_features_to_keep)\n",
    "        X_train_selected = fs.fit_transform(X_train_clustered_.values, y_train_)\n",
    "        top_features = fs.top_features[0:n_features_to_keep]\n",
    "        for j in top_features:\n",
    "            if Train.shape == (0,0):\n",
    "                Train = X_train_[[find_column(X_train_, X_train_clustered_, j,Train.columns)]]\n",
    "            else:\n",
    "                Train = Train.join(X_train_[[find_column(X_train_, X_train_clustered_, j,Train.columns)]])\n",
    "    return Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_dict(data, mutual_info):\n",
    "    dict = {}\n",
    "    for i,j in zip(data.columns,mutual_info):\n",
    "        dict[i] = j\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(X_train_transpose_clusteredx, X_train_,y_train_, label_cluster):\n",
    "    n_features_to_keep = 8\n",
    "    n_neighbors = X_train_.shape[0]-1\n",
    "    Train = pd.DataFrame()\n",
    "    for i in label_cluster:\n",
    "        X_train_transpose_clustered_ = X_train_transpose_clusteredx[X_train_transpose_clusteredx['cluster']==i]\n",
    "        X_train_clustered_ = X_train_transpose_clustered_.drop(['cluster'],axis=1).transpose()\n",
    "        MI = mutual_info_classif(X_train_clustered_,y_train_,n_neighbors=n_neighbors)\n",
    "        dict = create_features_dict(X_train_clustered_,MI)\n",
    "        X_train_selected = sorted(dict.items(), key = lambda kv:(kv[1], kv[0]),reverse=True)\n",
    "        X_train_selected = X_train_selected[:n_features_to_keep]\n",
    "        top_features = [i[0] for i in X_train_selected]\n",
    "        for j in top_features:\n",
    "            if Train.shape == (0,0):\n",
    "                Train = X_train_[[j]]\n",
    "            else:\n",
    "                Train = Train.join(X_train_[[j]])\n",
    "    return Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def optimum_components(X_train_):\n",
    "    n_components = min(X_train_.shape)\n",
    "    treshold = 0.90\n",
    "    sum = 0\n",
    "    components = 1\n",
    "    for i in range(1,n_components+1):\n",
    "        pca = PCA(n_components=i)\n",
    "        X_train_selected = pca.fit_transform(X_train_)\n",
    "        sum = pca.explained_variance_ratio_.sum()\n",
    "        components = i\n",
    "        if sum>treshold:\n",
    "            break\n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X_train_transpose_clusteredx, X_train_,y_train_, label_cluster):\n",
    "    Train = pd.DataFrame()\n",
    "    n_components = 1\n",
    "    n_features_to_keep = 8\n",
    "    for i in label_cluster:\n",
    "        X_train_transpose_clustered_ = X_train_transpose_clusteredx[X_train_transpose_clusteredx['cluster']==i]\n",
    "        X_train_clustered_ = X_train_transpose_clustered_.drop(['cluster'],axis=1).transpose()\n",
    "        pca = PCA(n_components=n_components)\n",
    "        ft = pca.fit_transform(X_train_clustered_)\n",
    "        X_train_selected = pca.components_[0]\n",
    "        dict = create_features_dict(X_train_clustered_,X_train_selected)\n",
    "        X_train_selected = sorted(dict.items(), key = lambda kv:(kv[1], kv[0]), reverse=True)\n",
    "        X_train_selected = X_train_selected[:n_features_to_keep]\n",
    "        top_features = [i[0] for i in X_train_selected]\n",
    "        for j in top_features:\n",
    "            if Train.shape == (0,0):\n",
    "                Train = X_train_[[j]]\n",
    "            else:\n",
    "                Train = Train.join(X_train_[[j]])\n",
    "    return Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(Train_,X_test_):\n",
    "    X_train_ = Train_.copy()\n",
    "    X_test_ = X_test_[X_train_.columns]\n",
    "    return X_train_,X_test_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rfg = RandomForestClassifier(criterion='gini', n_estimators=60)\n",
    "model_svm = SVC(kernel='linear')\n",
    "model_lr = LogisticRegression()\n",
    "model_knn = KNeighborsClassifier()\n",
    "model_mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = X.values\n",
    "y_ = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-fold Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=8, random_state=20)\n",
    "kf.get_n_splits(X_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Random Forest***\n",
    "\n",
    "- Relief Method with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=8, random_state=20, shuffle=False)\n",
      "Score in Random Forest :  0.875\n",
      "Score in Random Forest :  0.75\n",
      "Score in Random Forest :  0.875\n",
      "Score in Random Forest :  1.0\n",
      "Score in Random Forest :  0.875\n",
      "Score in Random Forest :  0.875\n",
      "Score in Random Forest :  0.5714285714285714\n",
      "Score in Random Forest :  0.8571428571428571\n",
      "avg accuraccy =  0.8348214285714286\n",
      "max accuraccy =  1.0\n",
      "min accuraccy =  0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(kf)\n",
    "scoring = []\n",
    "for train_index, test_index in kf.split(X_):\n",
    "    X_train, X_test = X_[train_index], X_[test_index]\n",
    "    y_train, y_test = y_[train_index], y_[test_index]\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "    X_train, X_test = minmax_scaler(X_train, X_test)\n",
    "    X_train_transpose_clustered = k_means(X_train)\n",
    "    clusters = set(X_train_transpose_clustered['cluster'].values.tolist())\n",
    "    Train = relief_method(X_train_transpose_clustered, X_train, y_train, clusters)\n",
    "    X_train, X_test = prepare_data(Train,X_test)\n",
    "    model_rfg.fit(X_train,y_train)\n",
    "    pred = model_rfg.predict(X_test)\n",
    "    hasil = accuracy_score(y_test, pred)\n",
    "    print(\"Score in Random Forest : \", hasil)\n",
    "    scoring.append(hasil)\n",
    "scoring = np.array(scoring)\n",
    "avg_rfg_RM = scoring.mean()\n",
    "print(\"avg accuraccy = \",avg_rfg_RM)\n",
    "print(\"max accuraccy = \",scoring.max())\n",
    "print(\"min accuraccy = \",scoring.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mutual Information with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=8, random_state=20, shuffle=False)\n",
      "Score in Random Forest :  0.875\n",
      "Score in Random Forest :  0.875\n",
      "Score in Random Forest :  0.75\n",
      "Score in Random Forest :  1.0\n",
      "Score in Random Forest :  1.0\n",
      "Score in Random Forest :  0.875\n",
      "Score in Random Forest :  0.5714285714285714\n",
      "Score in Random Forest :  0.7142857142857143\n",
      "avg accuraccy =  0.8325892857142857\n",
      "max accuraccy =  1.0\n",
      "min accuraccy =  0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(kf)\n",
    "scoring = []\n",
    "for train_index, test_index in kf.split(X_):\n",
    "    X_train, X_test = X_[train_index], X_[test_index]\n",
    "    y_train, y_test = y_[train_index], y_[test_index]\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "    X_train, X_test = minmax_scaler(X_train, X_test)\n",
    "    X_train_transpose_clustered = k_means(X_train)\n",
    "    clusters = set(X_train_transpose_clustered['cluster'].values.tolist())\n",
    "    Train = mutual_information(X_train_transpose_clustered, X_train, y_train, clusters)\n",
    "    X_train, X_test = prepare_data(Train,X_test)\n",
    "    model_rfg.fit(X_train,y_train)\n",
    "    pred = model_rfg.predict(X_test)\n",
    "    hasil = accuracy_score(y_test, pred)\n",
    "    print(\"Score in Random Forest : \", hasil)\n",
    "    scoring.append(hasil)\n",
    "scoring = np.array(scoring)\n",
    "avg_rfg_MI = scoring.mean()\n",
    "print(\"avg accuraccy = \",avg_rfg_MI)\n",
    "print(\"max accuraccy = \",scoring.max())\n",
    "print(\"min accuraccy = \",scoring.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=8, random_state=20, shuffle=False)\n",
      "Score in Random Forest :  0.625\n",
      "Score in Random Forest :  0.625\n",
      "Score in Random Forest :  0.75\n",
      "Score in Random Forest :  0.875\n",
      "Score in Random Forest :  1.0\n",
      "Score in Random Forest :  0.875\n",
      "Score in Random Forest :  0.5714285714285714\n",
      "Score in Random Forest :  0.7142857142857143\n",
      "avg accuraccy =  0.7544642857142857\n",
      "max accuraccy =  1.0\n",
      "min accuraccy =  0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(kf)\n",
    "scoring = []\n",
    "for train_index, test_index in kf.split(X_):\n",
    "    X_train, X_test = X_[train_index], X_[test_index]\n",
    "    y_train, y_test = y_[train_index], y_[test_index]\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "    X_train, X_test = minmax_scaler(X_train, X_test)\n",
    "    X_train_transpose_clustered = k_means(X_train)\n",
    "    clusters = set(X_train_transpose_clustered['cluster'].values.tolist())\n",
    "    Train = pca(X_train_transpose_clustered, X_train, y_train, clusters)\n",
    "    X_train, X_test = prepare_data(Train,X_test)\n",
    "    model_rfg.fit(X_train,y_train)\n",
    "    pred = model_rfg.predict(X_test)\n",
    "    hasil = accuracy_score(y_test, pred)\n",
    "    print(\"Score in Random Forest : \", hasil)\n",
    "    scoring.append(hasil)\n",
    "scoring = np.array(scoring)\n",
    "avg_rfg_PCA = scoring.mean()\n",
    "print(\"avg accuraccy = \",avg_rfg_PCA)\n",
    "print(\"max accuraccy = \",scoring.max())\n",
    "print(\"min accuraccy = \",scoring.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Support Vector Machine***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Relief Method with Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=8, random_state=20, shuffle=False)\n",
      "Score in SVM :  0.75\n",
      "Score in SVM :  0.75\n",
      "Score in SVM :  0.5\n",
      "Score in SVM :  1.0\n",
      "Score in SVM :  1.0\n",
      "Score in SVM :  0.875\n",
      "Score in SVM :  0.5714285714285714\n",
      "Score in SVM :  0.8571428571428571\n",
      "avg accuraccy =  0.7879464285714286\n",
      "max accuraccy =  1.0\n",
      "min accuraccy =  0.5\n"
     ]
    }
   ],
   "source": [
    "print(kf)\n",
    "scoring = []\n",
    "for train_index, test_index in kf.split(X_):\n",
    "    X_train, X_test = X_[train_index], X_[test_index]\n",
    "    y_train, y_test = y_[train_index], y_[test_index]\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "    X_train, X_test = minmax_scaler(X_train, X_test)\n",
    "    X_train_transpose_clustered = k_means(X_train)\n",
    "    clusters = set(X_train_transpose_clustered['cluster'].values.tolist())\n",
    "    Train = relief_method(X_train_transpose_clustered, X_train, y_train, clusters)\n",
    "    X_train, X_test = prepare_data(Train,X_test)\n",
    "    model_svm.fit(X_train,y_train)\n",
    "    pred = model_svm.predict(X_test)\n",
    "    hasil = accuracy_score(y_test, pred)\n",
    "    print(\"Score in SVM : \", hasil)\n",
    "    scoring.append(hasil)\n",
    "scoring = np.array(scoring)\n",
    "avg_svm_RM = scoring.mean()\n",
    "print(\"avg accuraccy = \",scoring.mean())\n",
    "print(\"max accuraccy = \",scoring.max())\n",
    "print(\"min accuraccy = \",scoring.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mutual information with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=8, random_state=20, shuffle=False)\n",
      "Score in SVM :  0.875\n",
      "Score in SVM :  0.875\n",
      "Score in SVM :  1.0\n",
      "Score in SVM :  1.0\n",
      "Score in SVM :  1.0\n",
      "Score in SVM :  0.75\n",
      "Score in SVM :  0.5714285714285714\n",
      "Score in SVM :  0.7142857142857143\n",
      "avg accuraccy =  0.8482142857142857\n",
      "max accuraccy =  1.0\n",
      "min accuraccy =  0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(kf)\n",
    "scoring = []\n",
    "for train_index, test_index in kf.split(X_):\n",
    "    X_train, X_test = X_[train_index], X_[test_index]\n",
    "    y_train, y_test = y_[train_index], y_[test_index]\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "    X_train, X_test = minmax_scaler(X_train, X_test)\n",
    "    X_train_transpose_clustered = k_means(X_train)\n",
    "    clusters = set(X_train_transpose_clustered['cluster'].values.tolist())\n",
    "    Train = mutual_information(X_train_transpose_clustered, X_train, y_train, clusters)\n",
    "    X_train, X_test = prepare_data(Train,X_test)\n",
    "    model_svm.fit(X_train,y_train)\n",
    "    pred = model_svm.predict(X_test)\n",
    "    hasil = accuracy_score(y_test, pred)\n",
    "    print(\"Score in SVM : \", hasil)\n",
    "    scoring.append(hasil)\n",
    "scoring = np.array(scoring)\n",
    "avg_svm_MI = scoring.mean()\n",
    "print(\"avg accuraccy = \",scoring.mean())\n",
    "print(\"max accuraccy = \",scoring.max())\n",
    "print(\"min accuraccy = \",scoring.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=8, random_state=20, shuffle=False)\n",
      "Score in SVM :  0.625\n",
      "Score in SVM :  0.625\n",
      "Score in SVM :  0.625\n",
      "Score in SVM :  0.875\n",
      "Score in SVM :  1.0\n",
      "Score in SVM :  0.875\n",
      "Score in SVM :  0.5714285714285714\n",
      "Score in SVM :  0.8571428571428571\n",
      "avg accuraccy =  0.7566964285714286\n",
      "max accuraccy =  1.0\n",
      "min accuraccy =  0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(kf)\n",
    "scoring = []\n",
    "for train_index, test_index in kf.split(X_):\n",
    "    X_train, X_test = X_[train_index], X_[test_index]\n",
    "    y_train, y_test = y_[train_index], y_[test_index]\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "    X_train, X_test = minmax_scaler(X_train, X_test)\n",
    "    X_train_transpose_clustered = k_means(X_train)\n",
    "    clusters = set(X_train_transpose_clustered['cluster'].values.tolist())\n",
    "    Train = pca(X_train_transpose_clustered, X_train, y_train, clusters)\n",
    "    X_train, X_test = prepare_data(Train,X_test)\n",
    "    model_svm.fit(X_train,y_train)\n",
    "    pred = model_svm.predict(X_test)\n",
    "    hasil = accuracy_score(y_test, pred)\n",
    "    print(\"Score in SVM : \", hasil)\n",
    "    scoring.append(hasil)\n",
    "scoring = np.array(scoring)\n",
    "avg_svm_PCA = scoring.mean()\n",
    "print(\"avg accuraccy = \",scoring.mean())\n",
    "print(\"max accuraccy = \",scoring.max())\n",
    "print(\"min accuraccy = \",scoring.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Logistic Regression***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Relief Method with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=8, random_state=20, shuffle=False)\n",
      "Score in Logistic Regression :  0.75\n",
      "Score in Logistic Regression :  0.625\n",
      "Score in Logistic Regression :  0.75\n",
      "Score in Logistic Regression :  1.0\n",
      "Score in Logistic Regression :  0.875\n",
      "Score in Logistic Regression :  1.0\n",
      "Score in Logistic Regression :  0.5714285714285714\n",
      "Score in Logistic Regression :  0.7142857142857143\n",
      "avg accuraccy =  0.7857142857142857\n",
      "max accuraccy =  1.0\n",
      "min accuraccy =  0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(kf)\n",
    "scoring = []\n",
    "for train_index, test_index in kf.split(X_):\n",
    "    X_train, X_test = X_[train_index], X_[test_index]\n",
    "    y_train, y_test = y_[train_index], y_[test_index]\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "    X_train, X_test = minmax_scaler(X_train, X_test)\n",
    "    X_train_transpose_clustered = k_means(X_train)\n",
    "    clusters = set(X_train_transpose_clustered['cluster'].values.tolist())\n",
    "    Train = relief_method(X_train_transpose_clustered, X_train, y_train, clusters)\n",
    "    X_train, X_test = prepare_data(Train,X_test)\n",
    "    model_lr.fit(X_train,y_train)\n",
    "    pred = model_lr.predict(X_test)\n",
    "    hasil = accuracy_score(y_test, pred)\n",
    "    print(\"Score in Logistic Regression : \", hasil)\n",
    "    scoring.append(hasil)\n",
    "scoring = np.array(scoring)\n",
    "avg_lr_RM = scoring.mean()\n",
    "print(\"avg accuraccy = \",avg_lr_RM)\n",
    "print(\"max accuraccy = \",scoring.max())\n",
    "print(\"min accuraccy = \",scoring.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mutual Information with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=8, random_state=20, shuffle=False)\n",
      "Score in Logistic Regression :  0.875\n",
      "Score in Logistic Regression :  0.875\n",
      "Score in Logistic Regression :  1.0\n",
      "Score in Logistic Regression :  1.0\n",
      "Score in Logistic Regression :  1.0\n",
      "Score in Logistic Regression :  0.875\n",
      "Score in Logistic Regression :  0.5714285714285714\n",
      "Score in Logistic Regression :  0.7142857142857143\n",
      "avg accuraccy =  0.8638392857142857\n",
      "max accuraccy =  1.0\n",
      "min accuraccy =  0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(kf)\n",
    "scoring = []\n",
    "for train_index, test_index in kf.split(X_):\n",
    "    X_train, X_test = X_[train_index], X_[test_index]\n",
    "    y_train, y_test = y_[train_index], y_[test_index]\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "    X_train, X_test = minmax_scaler(X_train, X_test)\n",
    "    X_train_transpose_clustered = k_means(X_train)\n",
    "    clusters = set(X_train_transpose_clustered['cluster'].values.tolist())\n",
    "    Train = mutual_information(X_train_transpose_clustered, X_train, y_train, clusters)\n",
    "    X_train, X_test = prepare_data(Train,X_test)\n",
    "    model_lr.fit(X_train,y_train)\n",
    "    pred = model_lr.predict(X_test)\n",
    "    hasil = accuracy_score(y_test, pred)\n",
    "    print(\"Score in Logistic Regression : \", hasil)\n",
    "    scoring.append(hasil)\n",
    "scoring = np.array(scoring)\n",
    "avg_lr_MI = scoring.mean()\n",
    "print(\"avg accuraccy = \",avg_lr_MI)\n",
    "print(\"max accuraccy = \",scoring.max())\n",
    "print(\"min accuraccy = \",scoring.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=8, random_state=20, shuffle=False)\n",
      "Score in Logistic Regression :  0.75\n",
      "Score in Logistic Regression :  0.625\n",
      "Score in Logistic Regression :  0.625\n",
      "Score in Logistic Regression :  1.0\n",
      "Score in Logistic Regression :  1.0\n",
      "Score in Logistic Regression :  0.875\n",
      "Score in Logistic Regression :  0.5714285714285714\n",
      "Score in Logistic Regression :  0.7142857142857143\n",
      "avg accuraccy =  0.7700892857142857\n",
      "max accuraccy =  1.0\n",
      "min accuraccy =  0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(kf)\n",
    "scoring = []\n",
    "for train_index, test_index in kf.split(X_):\n",
    "    X_train, X_test = X_[train_index], X_[test_index]\n",
    "    y_train, y_test = y_[train_index], y_[test_index]\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "    X_train, X_test = minmax_scaler(X_train, X_test)\n",
    "    X_train_transpose_clustered = k_means(X_train)\n",
    "    clusters = set(X_train_transpose_clustered['cluster'].values.tolist())\n",
    "    Train = pca(X_train_transpose_clustered, X_train, y_train, clusters)\n",
    "    X_train, X_test = prepare_data(Train,X_test)\n",
    "    model_lr.fit(X_train,y_train)\n",
    "    pred = model_lr.predict(X_test)\n",
    "    hasil = accuracy_score(y_test, pred)\n",
    "    print(\"Score in Logistic Regression : \", hasil)\n",
    "    scoring.append(hasil)\n",
    "scoring = np.array(scoring)\n",
    "avg_lr_PCA = scoring.mean()\n",
    "print(\"avg accuraccy = \",avg_lr_PCA)\n",
    "print(\"max accuraccy = \",scoring.max())\n",
    "print(\"min accuraccy = \",scoring.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***K nearest neighbors***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- relief method with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=8, random_state=20, shuffle=False)\n",
      "Score in KNN :  0.75\n",
      "Score in KNN :  0.625\n",
      "Score in KNN :  0.75\n",
      "Score in KNN :  0.625\n",
      "Score in KNN :  0.875\n",
      "Score in KNN :  0.875\n",
      "Score in KNN :  0.5714285714285714\n",
      "Score in KNN :  1.0\n",
      "avg accuraccy =  0.7589285714285714\n",
      "max accuraccy =  1.0\n",
      "min accuraccy =  0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(kf)\n",
    "scoring = []\n",
    "for train_index, test_index in kf.split(X_):\n",
    "    X_train, X_test = X_[train_index], X_[test_index]\n",
    "    y_train, y_test = y_[train_index], y_[test_index]\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "    X_train, X_test = minmax_scaler(X_train, X_test)\n",
    "    X_train_transpose_clustered = k_means(X_train)\n",
    "    clusters = set(X_train_transpose_clustered['cluster'].values.tolist())\n",
    "    Train = relief_method(X_train_transpose_clustered, X_train, y_train, clusters)\n",
    "    X_train, X_test = prepare_data(Train,X_test)\n",
    "    model_knn.fit(X_train,y_train)\n",
    "    pred = model_knn.predict(X_test)\n",
    "    hasil = accuracy_score(y_test, pred)\n",
    "    print(\"Score in KNN : \", hasil)\n",
    "    scoring.append(hasil)\n",
    "scoring = np.array(scoring)\n",
    "avg_knn_RM = scoring.mean()\n",
    "print(\"avg accuraccy = \",avg_knn_RM)\n",
    "print(\"max accuraccy = \",scoring.max())\n",
    "print(\"min accuraccy = \",scoring.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mutual information with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=8, random_state=20, shuffle=False)\n",
      "Score in KNN :  0.75\n",
      "Score in KNN :  0.875\n",
      "Score in KNN :  0.75\n",
      "Score in KNN :  1.0\n",
      "Score in KNN :  1.0\n",
      "Score in KNN :  0.75\n",
      "Score in KNN :  0.5714285714285714\n",
      "Score in KNN :  0.7142857142857143\n",
      "avg accuraccy =  0.8013392857142857\n",
      "max accuraccy =  1.0\n",
      "min accuraccy =  0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(kf)\n",
    "scoring = []\n",
    "for train_index, test_index in kf.split(X_):\n",
    "    X_train, X_test = X_[train_index], X_[test_index]\n",
    "    y_train, y_test = y_[train_index], y_[test_index]\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "    X_train, X_test = minmax_scaler(X_train, X_test)\n",
    "    X_train_transpose_clustered = k_means(X_train)\n",
    "    clusters = set(X_train_transpose_clustered['cluster'].values.tolist())\n",
    "    Train = mutual_information(X_train_transpose_clustered, X_train, y_train, clusters)\n",
    "    X_train, X_test = prepare_data(Train,X_test)\n",
    "    model_knn.fit(X_train,y_train)\n",
    "    pred = model_knn.predict(X_test)\n",
    "    hasil = accuracy_score(y_test, pred)\n",
    "    print(\"Score in KNN : \", hasil)\n",
    "    scoring.append(hasil)\n",
    "scoring = np.array(scoring)\n",
    "avg_knn_MI = scoring.mean()\n",
    "print(\"avg accuraccy = \",avg_knn_MI)\n",
    "print(\"max accuraccy = \",scoring.max())\n",
    "print(\"min accuraccy = \",scoring.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=8, random_state=20, shuffle=False)\n",
      "Score in KNN :  0.75\n",
      "Score in KNN :  0.375\n",
      "Score in KNN :  0.75\n",
      "Score in KNN :  0.875\n",
      "Score in KNN :  0.75\n",
      "Score in KNN :  1.0\n",
      "Score in KNN :  0.7142857142857143\n",
      "Score in KNN :  0.5714285714285714\n",
      "avg accuraccy =  0.7232142857142857\n",
      "max accuraccy =  1.0\n",
      "min accuraccy =  0.375\n"
     ]
    }
   ],
   "source": [
    "print(kf)\n",
    "scoring = []\n",
    "for train_index, test_index in kf.split(X_):\n",
    "    X_train, X_test = X_[train_index], X_[test_index]\n",
    "    y_train, y_test = y_[train_index], y_[test_index]\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "    X_train, X_test = minmax_scaler(X_train, X_test)\n",
    "    X_train_transpose_clustered = k_means(X_train)\n",
    "    clusters = set(X_train_transpose_clustered['cluster'].values.tolist())\n",
    "    Train = pca(X_train_transpose_clustered, X_train, y_train, clusters)\n",
    "    X_train, X_test = prepare_data(Train,X_test)\n",
    "    model_knn.fit(X_train,y_train)\n",
    "    pred = model_knn.predict(X_test)\n",
    "    hasil = accuracy_score(y_test, pred)\n",
    "    print(\"Score in KNN : \", hasil)\n",
    "    scoring.append(hasil)\n",
    "scoring = np.array(scoring)\n",
    "avg_knn_PCA = scoring.mean()\n",
    "print(\"avg accuraccy = \",avg_knn_PCA)\n",
    "print(\"max accuraccy = \",scoring.max())\n",
    "print(\"min accuraccy = \",scoring.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Multinomial naive bayes***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- relief method with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=8, random_state=20, shuffle=False)\n",
      "Score in Multinomial Naive Bayes :  0.625\n",
      "Score in Multinomial Naive Bayes :  0.5\n",
      "Score in Multinomial Naive Bayes :  0.625\n",
      "Score in Multinomial Naive Bayes :  1.0\n",
      "Score in Multinomial Naive Bayes :  0.875\n",
      "Score in Multinomial Naive Bayes :  0.875\n",
      "Score in Multinomial Naive Bayes :  0.5714285714285714\n",
      "Score in Multinomial Naive Bayes :  0.7142857142857143\n",
      "avg accuraccy =  0.7232142857142857\n",
      "max accuraccy =  1.0\n",
      "min accuraccy =  0.5\n"
     ]
    }
   ],
   "source": [
    "print(kf)\n",
    "scoring = []\n",
    "for train_index, test_index in kf.split(X_):\n",
    "    X_train, X_test = X_[train_index], X_[test_index]\n",
    "    y_train, y_test = y_[train_index], y_[test_index]\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "    X_train, X_test = minmax_scaler(X_train, X_test)\n",
    "    X_train_transpose_clustered = k_means(X_train)\n",
    "    clusters = set(X_train_transpose_clustered['cluster'].values.tolist())\n",
    "    Train = relief_method(X_train_transpose_clustered, X_train, y_train, clusters)\n",
    "    X_train, X_test = prepare_data(Train,X_test)\n",
    "    model_mnb.fit(X_train,y_train)\n",
    "    pred = model_mnb.predict(X_test)\n",
    "    hasil = accuracy_score(y_test, pred)\n",
    "    print(\"Score in Multinomial Naive Bayes : \", hasil)\n",
    "    scoring.append(hasil)\n",
    "scoring = np.array(scoring)\n",
    "avg_mnb_RM = scoring.mean()\n",
    "print(\"avg accuraccy = \",avg_mnb_RM)\n",
    "print(\"max accuraccy = \",scoring.max())\n",
    "print(\"min accuraccy = \",scoring.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mutual information with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=8, random_state=20, shuffle=False)\n",
      "Score in Multinomial Naive Bayes :  1.0\n",
      "Score in Multinomial Naive Bayes :  0.875\n",
      "Score in Multinomial Naive Bayes :  0.875\n",
      "Score in Multinomial Naive Bayes :  1.0\n",
      "Score in Multinomial Naive Bayes :  1.0\n",
      "Score in Multinomial Naive Bayes :  0.875\n",
      "Score in Multinomial Naive Bayes :  0.5714285714285714\n",
      "Score in Multinomial Naive Bayes :  0.8571428571428571\n",
      "avg accuraccy =  0.8816964285714286\n",
      "max accuraccy =  1.0\n",
      "min accuraccy =  0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(kf)\n",
    "scoring = []\n",
    "for train_index, test_index in kf.split(X_):\n",
    "    X_train, X_test = X_[train_index], X_[test_index]\n",
    "    y_train, y_test = y_[train_index], y_[test_index]\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "    X_train, X_test = minmax_scaler(X_train, X_test)\n",
    "    X_train_transpose_clustered = k_means(X_train)\n",
    "    clusters = set(X_train_transpose_clustered['cluster'].values.tolist())\n",
    "    Train = mutual_information(X_train_transpose_clustered, X_train, y_train, clusters)\n",
    "    X_train, X_test = prepare_data(Train,X_test)\n",
    "    model_mnb.fit(X_train,y_train)\n",
    "    pred = model_mnb.predict(X_test)\n",
    "    hasil = accuracy_score(y_test, pred)\n",
    "    print(\"Score in Multinomial Naive Bayes : \", hasil)\n",
    "    scoring.append(hasil)\n",
    "scoring = np.array(scoring)\n",
    "avg_mnb_MI = scoring.mean()\n",
    "print(\"avg accuraccy = \",avg_mnb_MI)\n",
    "print(\"max accuraccy = \",scoring.max())\n",
    "print(\"min accuraccy = \",scoring.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=8, random_state=20, shuffle=False)\n",
      "Score in Multinomial Naive Bayes :  0.625\n",
      "Score in Multinomial Naive Bayes :  0.625\n",
      "Score in Multinomial Naive Bayes :  0.625\n",
      "Score in Multinomial Naive Bayes :  0.875\n",
      "Score in Multinomial Naive Bayes :  1.0\n",
      "Score in Multinomial Naive Bayes :  0.875\n",
      "Score in Multinomial Naive Bayes :  0.5714285714285714\n",
      "Score in Multinomial Naive Bayes :  0.7142857142857143\n",
      "avg accuraccy =  0.7388392857142857\n",
      "max accuraccy =  1.0\n",
      "min accuraccy =  0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(kf)\n",
    "scoring = []\n",
    "for train_index, test_index in kf.split(X_):\n",
    "    X_train, X_test = X_[train_index], X_[test_index]\n",
    "    y_train, y_test = y_[train_index], y_[test_index]\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "    X_train, X_test = minmax_scaler(X_train, X_test)\n",
    "    X_train_transpose_clustered = k_means(X_train)\n",
    "    clusters = set(X_train_transpose_clustered['cluster'].values.tolist())\n",
    "    Train = pca(X_train_transpose_clustered, X_train, y_train, clusters)\n",
    "    X_train, X_test = prepare_data(Train,X_test)\n",
    "    model_mnb.fit(X_train,y_train)\n",
    "    pred = model_mnb.predict(X_test)\n",
    "    hasil = accuracy_score(y_test, pred)\n",
    "    print(\"Score in Multinomial Naive Bayes : \", hasil)\n",
    "    scoring.append(hasil)\n",
    "scoring = np.array(scoring)\n",
    "avg_mnb_PCA = scoring.mean()\n",
    "print(\"avg accuraccy = \",avg_mnb_PCA)\n",
    "print(\"max accuraccy = \",scoring.max())\n",
    "print(\"min accuraccy = \",scoring.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({\n",
    "    \"method\" : [\"Random Forest\",\"SVM\",\"Logistic Regression\",\"KNN\",\"Multinomial Naive Bayes\"],\n",
    "    \"Relief Method\" : [avg_rfg_RM,avg_svm_RM,avg_lr_RM,avg_knn_RM,avg_mnb_RM],\n",
    "    \"Mutual Information\" : [avg_rfg_MI,avg_svm_MI,avg_lr_MI,avg_knn_MI,avg_mnb_MI],\n",
    "    \"PCA\" : [avg_rfg_PCA,avg_svm_PCA,avg_lr_PCA,avg_knn_PCA,avg_mnb_PCA]\n",
    "})\n",
    "result = result.set_index(['method'],drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relief Method</th>\n",
       "      <th>Mutual Information</th>\n",
       "      <th>PCA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.834821</td>\n",
       "      <td>0.832589</td>\n",
       "      <td>0.754464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.787946</td>\n",
       "      <td>0.848214</td>\n",
       "      <td>0.756696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.863839</td>\n",
       "      <td>0.770089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.758929</td>\n",
       "      <td>0.801339</td>\n",
       "      <td>0.723214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multinomial Naive Bayes</th>\n",
       "      <td>0.723214</td>\n",
       "      <td>0.881696</td>\n",
       "      <td>0.738839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Relief Method  Mutual Information       PCA\n",
       "method                                                              \n",
       "Random Forest                 0.834821            0.832589  0.754464\n",
       "SVM                           0.787946            0.848214  0.756696\n",
       "Logistic Regression           0.785714            0.863839  0.770089\n",
       "KNN                           0.758929            0.801339  0.723214\n",
       "Multinomial Naive Bayes       0.723214            0.881696  0.738839"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result.loc[:,['Relief Method','Mutual Information','PCA']].plot.bar(color=['red','green','blue'], figsize=(20,10))\n",
    "plt.title(\"Perbandingan Akurasi\")\n",
    "plt.xlabel('Metode')\n",
    "plt.ylabel('Akurasi')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
